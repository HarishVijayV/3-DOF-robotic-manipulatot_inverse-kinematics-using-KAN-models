{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.07259460911154747, Test Loss: 0.10674368747406536, RMSE: 0.3651912808418274\n",
      "Epoch 20, Train Loss: 0.05361318092603805, Test Loss: 0.039542679105781846, RMSE: 0.22265422344207764\n",
      "Epoch 30, Train Loss: 0.06796069824173626, Test Loss: 0.050936807899011505, RMSE: 0.2525542378425598\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 184\u001b[0m\n\u001b[0;32m    181\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 184\u001b[0m     \u001b[43mdemo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 109\u001b[0m, in \u001b[0;36mdemo\u001b[1;34m()\u001b[0m\n\u001b[0;32m    107\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m fkan2(h)\n\u001b[0;32m    108\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y_batch)\n\u001b[1;32m--> 109\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    111\u001b[0m epoch_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\haris\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\haris\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define the Naive Fourier KAN Layer class\n",
    "class NaiveFourierKANLayer(th.nn.Module):\n",
    "    def __init__(self, inputdim, outdim, gridsize, addbias=True):\n",
    "        super(NaiveFourierKANLayer, self).__init__()\n",
    "        self.gridsize = gridsize\n",
    "        self.addbias = addbias\n",
    "        self.inputdim = inputdim\n",
    "        self.outdim = outdim\n",
    "        \n",
    "        self.fouriercoeffs = th.nn.Parameter(\n",
    "            th.randn(2, outdim, inputdim, gridsize) / (np.sqrt(inputdim) * np.sqrt(self.gridsize))\n",
    "        )\n",
    "        if self.addbias:\n",
    "            self.bias = th.nn.Parameter(th.zeros(1, outdim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        xshp = x.shape\n",
    "        outshape = xshp[0:-1] + (self.outdim,)\n",
    "        x = th.reshape(x, (-1, self.inputdim))\n",
    "        k = th.reshape(th.arange(1, self.gridsize + 1, device=x.device), (1, 1, 1, self.gridsize))\n",
    "        xrshp = th.reshape(x, (x.shape[0], 1, x.shape[1], 1))\n",
    "        c = th.cos(k * xrshp)\n",
    "        s = th.sin(k * xrshp)\n",
    "        y = th.sum(c * self.fouriercoeffs[0:1], (-2, -1))\n",
    "        y += th.sum(s * self.fouriercoeffs[1:2], (-2, -1))\n",
    "        if self.addbias:\n",
    "            y += self.bias\n",
    "        y = th.reshape(y, outshape)\n",
    "        return y\n",
    "\n",
    "def demo():\n",
    "    bs = 10\n",
    "    L = 3\n",
    "    \n",
    "    # Load data\n",
    "    data = pd.read_csv('RRR_data2.csv')\n",
    "    X = data[['px', 'py', 'pz']].values\n",
    "    y = data[['theta_1', 'theta_2', 'theta_3']].values\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    X = scaler_X.fit_transform(X)\n",
    "    y = scaler_y.fit_transform(y)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X = th.tensor(X, dtype=th.float32)\n",
    "    y = th.tensor(y, dtype=th.float32)\n",
    "    \n",
    "    # Create dataset and split into train and test sets\n",
    "    dataset = TensorDataset(X, y)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
    "    \n",
    "    # Model parameters\n",
    "    inputdim = 3\n",
    "    hidden = 130\n",
    "    outdim = 3\n",
    "    gridsize = 5\n",
    "    \n",
    "    device = \"cuda\" if th.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Initialize models\n",
    "    fkan1 = NaiveFourierKANLayer(inputdim, hidden, gridsize).to(device)\n",
    "    fkan2 = NaiveFourierKANLayer(hidden, outdim, gridsize).to(device)\n",
    "    \n",
    "    # Loss function\n",
    "    loss_fn = MSELoss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = Adam(list(fkan1.parameters()) + list(fkan2.parameters()), lr=1e-3)\n",
    "    \n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    epoch_rmse = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, 101):  # 100 epochs with a print every 10 epochs\n",
    "        fkan1.train()\n",
    "        fkan2.train()\n",
    "        epoch_train_loss = 0\n",
    "        start_time = time.time()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            h = fkan1(X_batch)\n",
    "            y_pred = fkan2(h)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "        \n",
    "        # Calculate average train loss\n",
    "        epoch_train_loss /= len(train_loader)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        fkan1.eval()\n",
    "        fkan2.eval()\n",
    "        epoch_test_loss = 0\n",
    "        all_y_true = []\n",
    "        all_y_pred = []\n",
    "        with th.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                h = fkan1(X_batch)\n",
    "                y_pred = fkan2(h)\n",
    "                epoch_test_loss += loss_fn(y_pred, y_batch).item()\n",
    "                all_y_true.append(y_batch.cpu().numpy())\n",
    "                all_y_pred.append(y_pred.cpu().numpy())\n",
    "        \n",
    "        # Calculate average test loss\n",
    "        epoch_test_loss /= len(test_loader)\n",
    "        test_losses.append(epoch_test_loss)\n",
    "\n",
    "        # Calculate RMSE\n",
    "        all_y_true = np.concatenate(all_y_true)\n",
    "        all_y_pred = np.concatenate(all_y_pred)\n",
    "        all_y_true = scaler_y.inverse_transform(all_y_true)\n",
    "        all_y_pred = scaler_y.inverse_transform(all_y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(all_y_true, all_y_pred))\n",
    "        epoch_rmse.append(rmse)\n",
    "\n",
    "        # Print epoch metrics\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Train Loss: {epoch_train_loss}, Test Loss: {epoch_test_loss}, RMSE: {rmse}')\n",
    "    \n",
    "    # Calculate final RMSE and R2 score on the whole dataset\n",
    "    fkan1.eval()\n",
    "    fkan2.eval()\n",
    "    with th.no_grad():\n",
    "        predictions = fkan2(fkan1(X.to(device)))\n",
    "        predictions = scaler_y.inverse_transform(predictions.cpu().numpy())\n",
    "        y_original = scaler_y.inverse_transform(y.cpu().numpy())\n",
    "        final_rmse = np.sqrt(mean_squared_error(y_original, predictions))\n",
    "        final_r2 = r2_score(y_original, predictions)\n",
    "        print(f'Final RMSE: {final_rmse}')\n",
    "        print(f'Final R2 Score: {final_r2}')\n",
    "    \n",
    "    # Plotting epoch vs loss and epoch vs RMSE\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Training and Testing Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(1, len(test_losses) + 1), test_losses, label='Testing Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Epoch vs Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot RMSE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, len(epoch_rmse) + 1), epoch_rmse, label='RMSE', marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.title('Epoch vs RMSE')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
